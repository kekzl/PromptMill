"""Generate prompt use case."""

from collections.abc import Iterator
from dataclasses import dataclass

from promptmill.domain.exceptions import ModelNotLoadedError, RoleNotFoundError
from promptmill.domain.ports.llm_port import LLMPort
from promptmill.domain.ports.role_repository_port import RoleRepositoryPort
from promptmill.domain.value_objects.prompt_request import PromptGenerationRequest


@dataclass(slots=True)
class GeneratePromptUseCase:
    """Use case for generating prompts using the loaded LLM.

    This use case orchestrates the prompt generation process by:
    1. Retrieving the appropriate role
    2. Validating the request
    3. Invoking the LLM for generation
    """

    llm: LLMPort
    role_repository: RoleRepositoryPort

    def execute(self, request: PromptGenerationRequest) -> Iterator[str]:
        """Execute the prompt generation use case.

        Args:
            request: The prompt generation request containing user input and parameters.

        Yields:
            Text chunks as they are generated by the LLM.

        Raises:
            RoleNotFoundError: If the requested role doesn't exist.
            ModelNotLoadedError: If no model is loaded.
        """
        # Verify model is loaded
        if not self.llm.is_loaded():
            raise ModelNotLoadedError()

        # Get the role
        role = self.role_repository.get_by_display_name(request.role_display_name)
        if role is None:
            raise RoleNotFoundError(request.role_display_name)

        # Generate using the LLM
        yield from self.llm.generate(
            system_prompt=role.system_prompt,
            user_prompt=request.stripped_input,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
        )
